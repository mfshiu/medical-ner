import os
import sys
import numpy as np
# import pandas as pd
# from tqdm import tqdm
from ckiptagger import construct_dictionary, WS
from common import util


NAME_ENTITY_MARK = "@N^E_M@"
QUESTION_SIZE = 29


def loadInputFile(file_path):
    trainingset = list()  # store trainingset [content,content,...]
    position = list()  # store position [article_id, start_pos, end_pos, entity_text, entity_type, ...]
    mentions = dict()  # store mentions[mention] = Type
    with open(file_path, 'r', encoding='utf8') as f:
        file_text = f.read().encode('utf-8').decode('utf-8-sig')
    datas = file_text.split('\n\n--------------------\n\n')[:-1]
    for data in datas:
        data = data.split('\n')
        content = data[0]
        trainingset.append(content)
        annotations = data[1:]
        for annot in annotations[1:]:
            annot = annot.split('\t')  # annot= article_id, start_pos, end_pos, entity_text, entity_type
            position.extend(annot)
            mentions[annot[3]] = annot[4]

    return trainingset, position, mentions


def load_coerce_dictionary(custom_dict=None):
    file_path = 'dataset/dict_coerce.txt'
    coerce_dict = util.load_dictionary(file_path)
    if custom_dict:
        coerce_dict.update(custom_dict)

    return coerce_dict


global _recommend_dict
_recommend_dict = None

def load_recommend_dictionary(custom_dict=None):
    global _recommend_dict

    if _recommend_dict:
        return _recommend_dict

    file_path = root_dir + '/dataset/dict_recommend.txt'
    _recommend_dict = util.load_dictionary(file_path)

    time_dict = dict([(a, 'time') for a in util.get_time_entities()])
    _recommend_dict.update(time_dict)

    if custom_dict:
        _recommend_dict.update(custom_dict)

    return _recommend_dict


if __name__ == '__main__':
    if len(sys.argv) != 3:
        print("Error: need to specify source file name and output file name")
        print("Usage: python convert.py source_file output_file named_entities_file")
        sys.exit(-1)

    source_file = sys.argv[1]
    output_file = sys.argv[2]

    default_name_entities = dict([(a, 'time') for a in util.get_time_entities()])
    articles, position, mentions = loadInputFile(source_file)

    print("Segmenting...", end=' ')
    ws = WS("./ckipdata")
    recommend_dict = dict([(k, 1) for k in load_recommend_dictionary()])
    coerce_dict = dict([(k, 1) for k in load_coerce_dictionary()])
    article_words = ws(articles,
                       recommend_dictionary=util.construct_dictionary(recommend_dict),
                       coerce_dictionary=util.construct_dictionary(coerce_dict))
    print("done.")

    print("Geterating data...")
    train_data = []
    for word in recommend_dict:
        train_data.append(word.ljust(QUESTION_SIZE) + "_" + recommend_dict[word] + "\n")
    for word in coerce_dict:
        train_data.append(word.ljust(QUESTION_SIZE) + "_" + recommend_dict[word] + "\n")

    for i, words in enumerate(article_words):
        print("[%d] %s..." % (i, words[:10]))
        for s in words:
            if NAME_ENTITY_MARK != s:
                if s in default_name_entities:
                    train_data.append(s.ljust(29) + "_" + default_name_entities[s] + "\n")
                else:
                    train_data.append(s.ljust(29) + "_" + "O" + "\n")
    train_data[-1] = train_data[-1][:-1]
    print("done.")

    print("Writing to output file...", end=' ')
    with open(output_file, 'w', encoding='utf8') as fp:
        fp.writelines(train_data)
    print("done.")

    print("Writing to name entities file...", end=' ')
    default_name_entities["個管師"] = "O"
    lines = [k + " " + v + "\n" for k, v in default_name_entities.items()]
    lines[-1] = lines[-1][:-1]
    with open(named_entities_file, 'w', encoding='utf8') as fp:
        fp.writelines(lines)
    print("done.")

一下 none
一次 none
一定 none
一些 none
一半 none
一個 none
一樣 none
一跳 none
一直 none
一陣 none
這個 none
第一 none
IG contact
一般 none
一點點 none
陽明 location
台大 location
慶鴻 location
黃醫師 name
醫師 none
顆 none
陽明 location
慶鴻 location